---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "Andrew Guerra, Pratyush Rallapally, Satvik Talchuru, Akhil Gorla, Anishkumar Senthil"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

```{r, message=FALSE}
# load any other packages and read data here
library(tidyverse)
library(here)
library(readr)
library(knitr)
library(dplyr)
raw.biomarkers <- read_csv(here("data", "biomarker-raw.csv"))

```

## Abstract

Write a brief one-paragraph abstract that describes the contents of your write-up.

## Dataset

The dataset originates from Hewitson et al. (2021), Blood biomarker discovery for autism spectrum disorder. The data were obtained from serum samples of 154 male children aged from 18 months to 8 years, including 76 boys diagnosed with autism spectrum disorder (ASD) and 78 typically developing (TD) controls. All participants were recruited through The Johnson Center for Child Health and Development in Austin, TX, and ASD diagnoses were confirmed via the Autism Diagnostic Observation Schedule (ADOS) and Autism Diagnostic Interview-Revised (ADI-R) under DSM-5 criteria. TD children were screened with the Adaptive Behavior Assessment System (ABAS-II) to rule out developmental concerns.

Blood samples were collected after participants fasted, then processed under controlled lab conditions. Protein levels were measured using the SomaLogic SOMAScan platform, which originally captured 1,317 proteins per sample. After running quality control checks, 1,125 proteins were kept for analysis.

## Summary of published analysis

The research used a multi-step approach that combined advanced protein analysis with machine learning techniques to find the best set of protein biomarkers. Initially, serum samples from 76 boys with Autism Spectrum Disorder (ASD) and 78 typically developing (TD) boys were analyzed for proteins using the SomaLogic SOMAScan. This platform originally assessed 1,317 proteins, and after undergoing quality control, 1,125 proteins were analyzed further using bioinformatics techniques. The data on protein abundance were adjusted using log transformation and z-transformation prior to the start of prediction analysis.

To identify the most predictive proteins, 3 distinct computational approaches were used to independently determine the top-10 proteins: Random Forest (RF) analysis, employing MeanDecreaseGini to assess feature importance; t-test analysis, which detected proteins significantly differing between the ASD and TD groups; and a correlation method, which assessed the statistical connection between protein levels and ASD severity, as indicated by ADOS total scores. The proteins that were shared by all three methods were referred to as five "core proteins."

After identifying the main proteins, the researchers improved the predictive panel by utilizing a logistic regression model. The ability to predict was assessed by utilizing the Area Under the Curve measurement. This process included gradually incorporating the other non-core proteins into the core group to identify which additions enhanced predictive capability. The logistic regression model was trained by randomly assigning 80% of the subjects to a training dataset and 20% to a test dataset. This procedure was repeated 1,000 times to minimize bias and the optimal panel was made up of 9 proteins.

### Methodological Decisions and Approach

In our analysis of the ASD biomarker data, we made several key methodological decisions:

1.  Data Processing Choices:
    -   Retained log-transformation of protein levels to handle skewed distributions and varying scales
    -   Investigated the impact of outlier trimming on both individual proteins and subjects
    -   Used standardization (z-scores) for outlier detection across proteins
2.  Feature Selection Strategy:
    -   Expanded the protein panel beyond the original 10-protein limit to explore additional biomarkers
    -   Applied three complementary selection methods:
        -   T-tests for univariate differential expression
        -   Random Forest importance scores for capturing non-linear relationships
        -   LASSO regression for sparse feature selection with correlation consideration
    -   Set selection threshold at top 20 proteins per method to balance panel size with potential biomarker discovery
3.  Method Integration:
    -   Combined results across methods to identify consensus proteins
    -   Analyzed method agreement through overlap statistics
    -   Created visualizations to compare rankings across methods
    -   Used both statistical significance (t-tests) and predictive importance (RF, LASSO) metrics
4.  Validation Approach:
    -   Maintained separation between feature selection and model evaluation
    -   Used cross-validation in LASSO to avoid overfitting
    -   Set Random Forest parameters (ntree=1000) for stable importance measures
    -   Considered both individual protein significance and combined panel performance

These decisions were made to balance: - Discovery of novel biomarkers vs. risk of false positives - Model complexity vs. interpretability - Statistical rigor vs. practical clinical utility - Reproducibility vs. exploratory analysis

## Findings

Summarize your findings here. I've included some subheaders in a way that seems natural to me; you can structure this section however you like.

### Impact of preprocessing and outliers

**What do you imagine is the reason for log-transforming the protein levels in biomarker-raw.csv?**

Before protein levels are measured, a log-transformation is applied. The protein levels in biomarker-raw.csv are logarithmically transformed to better capture small variations in biomarker level expression. When working with microscopic data such as protein levels, the range of values is often dramatic. As a result, data can often be accentuated more or less and make overall trends harder to detect. To control for this, we can apply a log-transformation to standardize our data range.

To verify this, we can sample from 5 proteins in the dataset and see their respective distributions.

```{r, out.width='80%', fig.align='center'}
set.seed(10302025)
# Change Headers to Protein Acronyms
raw.biomarkers1 <- raw.biomarkers
colnames(raw.biomarkers1) <- raw.biomarkers1[1,]
raw.data <- raw.biomarkers1[-1,]
sample.proteins <- sample(colnames(raw.data), 5)
# RS of n=4 data
cat('Our proteins sampled are: ', sample.proteins)

# Checking distributions
long.data <- raw.data %>%
  select(all_of(sample.proteins)) %>%
  pivot_longer(cols = everything(), names_to = "Protein", values_to = "Value") %>%
  mutate(Value = as.numeric(Value))

# Histogram
long.data %>% 
  drop_na() %>% 
ggplot(aes(x = Value)) +
  geom_histogram(color = "white", fill = "black", bins = 30) +
  facet_wrap(~ Protein, scales = "free_x") +
  labs(
    title = "Distributions of Selected Protein Biomarkers",
    x = "Protein Level",
    y = "Frequency"
  ) +
  theme_minimal()

```

After plotting the histograms of our sampled proteins, we see that they most (except Apo E4) exhibit notable skewness and are not normally distribution. Additionally, the range for these values is extremely wide. Next, we apply a log-transformation to help standardize our distribtutions.

```{r, fig.align='center', out.width='80%'}
set.seed(10302025)

long.data %>% 
  drop_na() %>% 
ggplot(aes(x = log(Value))) +
  geom_histogram(color = "white", fill = "black", bins = 30) +
  facet_wrap(~ Protein, scales = "free_x") +
  labs(
    title = "Distributions of Selected Protein Biomarkers",
    x = "Log-Transformed Protein Level",
    y = "Frequency"
  ) +
  theme_minimal()
```

After transforming our values, we see that the range is now much considerably smaller. Additionally, most of our proteins sampled follow a close-to-normal distribution. Interestingly, Coagulation Factor IX appears to have a median of about 8.8. This trend was not as apparent with applying a transformation. Thus, we have reason to suspect that this protein should be further investigated.

**Temporarily remove the outlier trimming from preprocessing and do some exploratory analysis of the outlying values. Are there specific subjects (not values) that seem to be outliers? If so, are outliers more frequent in one group of the other?**

Through our previous analyses, we see that the distribution of proteins are transformed to easily detect which have a non-normal relationship that can be investigated further. Shifting our focus, we aim to see whether subjects are outliers. That is, we aim to see whether there are subjects, portrayed as rows, that consist of outliers among multiple protein levels.

```{r fig.align='center', warning=FALSE, out.width='80%'}
proteins <- setdiff(names(raw.biomarkers), 
                    c("Group", "Target Full Name"))

z.scores <- raw.biomarkers %>% 
  mutate(across(all_of(proteins), ~scale(as.numeric(.x))))


outlier_summary <- z.scores %>% 
  mutate(across(all_of(proteins), ~abs(.x) > 3)) %>% 
  mutate(n_outliers = rowSums(across(all_of(proteins)), na.rm = TRUE)) %>% 
  filter(!is.na(Group) & Group != '') %>%
  group_by(Group) %>% 
  summarise(
    mean_outliers = mean(n_outliers, na.rm = TRUE),
    median_outliers = median(n_outliers, na.rm = TRUE),
    sd_outliers = sd(n_outliers, na.rm = TRUE),
    max_outliers = max(n_outliers, na.rm = TRUE),
    .groups = "drop"
  )

outlier_summary %>% kable(caption = 'Outlier Distribution Table', digits=2)
```

We created a table to see outlier trends by subject group. To do so, we standardized all 1,317 protein levels and counted values with an absolute value greater than 3 as an outlier. Based on our table, we see that ASD subjects had an average of about 17.04 outliers in protein levels while TD subjects had about 19.65. Additionally, the median outlier counters were nearly identical (9.5 for ASD and 10 for TD), indicating that the two groups have similar overall variation in protein measurements. However, the standard deviations (20.6 for ASD and 30.4 for TD) and maximum outlier counts (119 and 154, respectively) suggest that a few individual subjects in both groups exhibited unusually high numbers of outlying protein values. Overall, this does not suggest there being systematic differences between groups, rather it suggests there are few subjects in the data that have greater individual differences.

### Methodological variations

3a) Train v. Test Set Analysis

```{r}
library(tidyverse)
library(tidymodels)
library(modelr)
library(rsample)
library(yardstick)

load("../data/biomarker-clean.RData")

biomarker <- biomarker_clean

# These are  5 proteins of interest that were used in the log regression example
s_star <- c("DERM", "RELT", "IgD", "PTN", "FSTL1")

biomarker <- biomarker %>%
  select(group, all_of(s_star)) %>%
  mutate(class = (group == "ASD")) %>%
  select(-group)

set.seed(123)

# 80% used for model fitting (training) and 20% held out for evaluation (testing).
partitions <- initial_split(biomarker, prop = 0.8)
train <- training(partitions)
test <- testing(partitions)

# Predicts the probability of ASD (class == TRUE) based on protein levels.
fit <- glm(class ~ ., data = train, family = binomial(link = "logit"))
summary(fit)

# Computing predictions on the test set
pred_df <- test %>%
  add_predictions(fit, type = "response") %>%
  mutate(pred_class = (pred > 0.5),
         group = factor(class, labels = c("TD", "ASD")),
         pred_group = factor(pred_class, labels = c("TD", "ASD")))

# Checking factor order
levels(pred_df$group)

# Defining evaluation metrics
panel_fn <- metric_set(accuracy, sensitivity, specificity, roc_auc)

results <- pred_df %>%
  panel_fn(truth = group,
           estimate = pred_group,
           pred,
           event_level = "second")

results
```

After conducting the logistic regression on a training subset (80%) and evaluating performance on an excluded test set, the model achieved an accuracy of 0.7097, sensitivity of 0.7222, specificity of 0.6923, and AUROC of 0.8291. These results show that when the model is tested on unseen data, it correctly classifies about 71% of subjects overall and the sensitivity value of 0.72 means the classifier correctly identifies roughly 72% of children with ASD, while the specificity of 0.69 means it correctly recognizes about 69 % of typically developing children.

Compared to the full-data analysis where accuracy ≈ 0.76 and AUC ≈ 0.83, these results show a small but average decline in performance due to testing on unseen data. We can infer that this decrease reflects reduced over-fitting and allows for a more accurate measure of the model’s generalization ability. Despite the decrease, the classifier maintains good discriminative power with AUC ≈ 0.83, establishing that the protein panel of DERM, RELT, IgD, PTN, and FSTL1 captures biological variation in relation to ASD versus TD subjects.

### 3b) Top 10 Predictive Proteins

To explore the impact of selecting more proteins for the biomarker panel, we expanded the selection criteria to identify the top 20 proteins using three different methods: t-tests for differential expression, Random Forest importance scores, and LASSO regression coefficients. This expansion allows us to evaluate whether including additional proteins could improve classification performance or reveal additional biomarkers of interest.

```{r load-biomarker-data}
load(here("data", "biomarker-clean.RData"))
```

```{r select-expanded-proteins}
top_n <- 20

dat <- biomarker_clean
dat$group <- factor(dat$group)
proteins <- setdiff(names(dat), c('group','ados'))

tt_res <- sapply(proteins, function(p){
  x <- dat[[p]]
  grp <- dat$group
  ok <- !is.na(x) & !is.na(grp)
  if(sum(ok) < 3) return(NA)
  t <- try(t.test(x[ok] ~ grp[ok]), silent=TRUE)
  if(inherits(t,'try-error')) return(NA)
  t$p.value
})
tt_df <- tibble::tibble(protein = proteins,
                       pvalue = as.numeric(tt_res)) %>%
  arrange(pvalue) %>%
  slice_head(n = top_n)

rf_dat <- dat %>% dplyr::select(dplyr::all_of(proteins))
rf_resp <- dat$group
rf_fit <- randomForest::randomForest(x = rf_dat, y = rf_resp, ntree = 1000, importance = TRUE)
rf_imp_mat <- randomForest::importance(rf_fit, type = 2)
rf_imp_val <- if(is.matrix(rf_imp_mat)) rf_imp_mat[,1] else rf_imp_mat
rf_df <- tibble::tibble(protein = names(rf_imp_val), importance = as.numeric(rf_imp_val)) %>%
  arrange(desc(importance)) %>%
  slice_head(n = top_n)

X <- as.matrix(rf_dat)
y <- as.numeric(dat$group) - 1
cv <- glmnet::cv.glmnet(X, y, family = 'binomial', alpha = 1, nfolds = 5)
coef_min <- as.matrix(coef(cv, s = 'lambda.min'))
coefs <- coef_min[-1,1]
lasso_df <- tibble::tibble(protein = proteins, coef = as.numeric(coefs)) %>%
  mutate(abscoef = abs(coef)) %>%
  arrange(desc(abscoef)) %>%
  slice_head(n = top_n)
```

```{r visualize-method-comparison, fig.width=10, fig.height=6}
method_comparison <- bind_rows(
  mutate(tt_df %>% mutate(rank = row_number()), method = "T-test"),
  mutate(rf_df %>% mutate(rank = row_number()), method = "Random Forest"),
  mutate(lasso_df %>% mutate(rank = row_number()), method = "LASSO")
)

ggplot(method_comparison, aes(x = rank, y = protein, color = method)) +
  geom_point(size = 3) +
  theme_minimal() +
  scale_x_continuous(breaks = 1:top_n) +
  labs(title = paste("Top", top_n, "Proteins Selected by Each Method"),
       x = "Rank within Method",
       y = "Protein",
       color = "Selection Method") +
  theme(legend.position = "bottom",
        axis.text.y = element_text(size = 8))
```

```{r protein-overlap}
tt_top <- tt_df$protein
rf_top <- rf_df$protein
lasso_top <- lasso_df$protein

all_methods <- intersect(intersect(tt_top, rf_top), lasso_top)

two_plus_methods <- unique(c(
  intersect(tt_top, rf_top),
  intersect(tt_top, lasso_top),
  intersect(rf_top, lasso_top)
))

overlap_summary <- tibble::tribble(
  ~"Overlap Type", ~"Count", ~"Percentage",
  "Selected by all methods", length(all_methods), length(all_methods)/top_n*100,
  "Selected by ≥2 methods", length(two_plus_methods), length(two_plus_methods)/top_n*100,
  "Unique proteins total", length(unique(c(tt_top, rf_top, lasso_top))), 
  length(unique(c(tt_top, rf_top, lasso_top)))/top_n*100
)

kable(overlap_summary, 
      caption = paste("Overlap Analysis of Top", top_n, "Proteins"),
      digits = 1)
```

Key findings from expanding the protein selection:

1.  **Method Agreement**: Among the top `r top_n` proteins, `r length(all_methods)` were identified by all three methods, suggesting strong consensus on these biomarkers. An additional `r length(two_plus_methods) - length(all_methods)` proteins were selected by at least two methods.

2.  **Unique Contributions**: Each method identified some proteins not found by the others, with a total of `r length(unique(c(tt_top, rf_top, lasso_top)))` unique proteins across all methods. This suggests that different statistical approaches may capture distinct aspects of the protein-ASD relationship.

3.  **Ranking Patterns**: The visualization shows that while some proteins are consistently ranked highly across methods, there is considerable variation in the rankings, particularly for proteins ranked lower in the top `r top_n`.

4.  **Implications**: The expanded panel reveals additional proteins that may be biologically relevant to ASD, though their predictive value would need to be validated in follow-up studies. The overlap analysis suggests that using multiple selection methods provides complementary information about potential biomarkers.

### Simplified classifier

```{r, echo = FALSE, message=FALSE}

kable(bind_rows(metrics_yours, metrics_full), caption = "Training vs Full-Data Performance", digits = 3)
```

To identify a simpler panel while maintaining comparable classification accuracy, we applied a training-only feature selection procedure using the same combination of multiple testing and random forest methods as in the in-class analysis. By restricting selection to the training set, we avoided using any test data during feature selection, ensuring a more methodologically sound and generalizable panel. This yielded a 4-protein panel, compared with the 5-core proteins identified in Hewitson et al.’s original study. The reduction of a single protein is a modest simplification, and despite this, the panel achieved strong predictive performance (sensitivity 0.75, specificity 0.80, accuracy 0.774, AUC 0.871). Benchmarking against the in-class analysis (sensitivity 0.875, specificity 0.867, accuracy 0.871, AUC 0.925) shows that the slight reduction in performance is minor relative to the substantial gain in interpretability and efficiency, supporting the utility of the smaller panel. By performing feature selection exclusively on training data, our panel avoids overfitting to the dataset and is more likely to generalize to new cohorts. Using a smaller panel with fewer proteins has several practical benefits, including cheaper and faster assays in a lab setting, culminating in easier replication.
