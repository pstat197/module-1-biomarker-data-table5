---
title: "Biomarkers of ASD"
subtitle: "If you want a subtitle put it here"
author: "Andrew Guerra, Pratyush Rallapally"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

Use this as a template. Keep the headers and remove all other text. In all, your report can be quite short. When it is complete, render and then push changes to your team repository.

```{r, message=FALSE}
# load any other packages and read data here
library(tidyverse)
library(here)
library(readr)
library(knitr)
library(dplyr)
raw.biomarkers <- read_csv(here("data", "biomarker-raw.csv"))

```

## Abstract

Write a brief one-paragraph abstract that describes the contents of your write-up.

## Dataset

The dataset originates from Hewitson et al. (2021), Blood biomarker discovery for autism spectrum disorder. The data were obtained from serum samples of 154 male children aged from 18 months to 8 years, including 76 boys diagnosed with autism spectrum disorder (ASD) and 78 typically developing (TD) controls. All participants were recruited through The Johnson Center for Child Health and Development in Austin, TX, and ASD diagnoses were confirmed via the Autism Diagnostic Observation Schedule (ADOS) and Autism Diagnostic Interview-Revised (ADI-R) under DSM-5 criteria. TD children were screened with the Adaptive Behavior Assessment System (ABAS-II) to rule out developmental concerns.

Blood samples were collected after participants fasted, then processed under controlled lab conditions. Protein levels were measured using the SomaLogic SOMAScan platform, which originally captured 1,317 proteins per sample. After running quality control checks, 1,125 proteins were kept for analysis.

## Summary of published analysis

Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for [GraphViz and Mermaid flowcharts](https://quarto.org/docs/authoring/diagrams.html).) Provide key results: the proteins selected for the classifier and the estimated accuracy.

## Findings

### Methodological Decisions and Approach

In our analysis of the ASD biomarker data, we made several key methodological decisions:

1.  **Data Processing Choices**:
    -   Retained log-transformation of protein levels to handle skewed distributions and varying scales
    -   Investigated the impact of outlier trimming on both individual proteins and subjects
    -   Used standardization (z-scores) for outlier detection across proteins
2.  **Feature Selection Strategy**:
    -   Expanded the protein panel beyond the original 10-protein limit to explore additional biomarkers
    -   Applied three complementary selection methods:
        -   T-tests for univariate differential expression
        -   Random Forest importance scores for capturing non-linear relationships
        -   LASSO regression for sparse feature selection with correlation consideration
    -   Set selection threshold at top 20 proteins per method to balance panel size with potential biomarker discovery
3.  **Method Integration**:
    -   Combined results across methods to identify consensus proteins
    -   Analyzed method agreement through overlap statistics
    -   Created visualizations to compare rankings across methods
    -   Used both statistical significance (t-tests) and predictive importance (RF, LASSO) metrics
4.  **Validation Approach**:
    -   Maintained separation between feature selection and model evaluation
    -   Used cross-validation in LASSO to avoid overfitting
    -   Set Random Forest parameters (ntree=1000) for stable importance measures
    -   Considered both individual protein significance and combined panel performance

These decisions were made to balance: - Discovery of novel biomarkers vs. risk of false positives - Model complexity vs. interpretability - Statistical rigor vs. practical clinical utility - Reproducibility vs. exploratory analysis

### Impact of preprocessing and outliers

**What do you imagine is the reason for log-transforming the protein levels in biomarker-raw.csv?**

Before protein levels are measured, a log-transformation is applied. The protein levels in biomarker-raw.csv are logarithmically transformed to better capture small variations in biomarker level expression. When working with microscopic data such as protein levels, the range of values is often dramatic. As a result, data can often be accentuated more or less and make overall trends harder to detect. To control for this, we can apply a log-transformation to standardize our data range.

To verify this, we can sample from 5 proteins in the dataset and see their respective distributions.

```{r, out.width='80%', fig.align='center'}
set.seed(10302025)
# Change Headers to Protein Acronyms
raw.biomarkers1 <- raw.biomarkers
colnames(raw.biomarkers1) <- raw.biomarkers1[1,]
raw.data <- raw.biomarkers1[-1,]
sample.proteins <- sample(colnames(raw.data), 5)
# RS of n=4 data
cat('Our proteins sampled are: ', sample.proteins)

# Checking distributions
long.data <- raw.data %>%
  select(all_of(sample.proteins)) %>%
  pivot_longer(cols = everything(), names_to = "Protein", values_to = "Value") %>%
  mutate(Value = as.numeric(Value))

# Histogram
long.data %>% 
  drop_na() %>% 
ggplot(aes(x = Value)) +
  geom_histogram(color = "white", fill = "black", bins = 30) +
  facet_wrap(~ Protein, scales = "free_x") +
  labs(
    title = "Distributions of Selected Protein Biomarkers",
    x = "Protein Level",
    y = "Frequency"
  ) +
  theme_minimal()

```

After plotting the histograms of our sampled proteins, we see that they most (except Apo E4) exhibit notable skewness and are not normally distribution. Additionally, the range for these values is extremely wide. Next, we apply a log-transformation to help standardize our distribtutions.

```{r, fig.align='center', out.width='80%'}
set.seed(10302025)

long.data %>% 
  drop_na() %>% 
ggplot(aes(x = log(Value))) +
  geom_histogram(color = "white", fill = "black", bins = 30) +
  facet_wrap(~ Protein, scales = "free_x") +
  labs(
    title = "Distributions of Selected Protein Biomarkers",
    x = "Log-Transformed Protein Level",
    y = "Frequency"
  ) +
  theme_minimal()
```

After transforming our values, we see that the range is now much considerably smaller. Additionally, most of our proteins sampled follow a close-to-normal distribution. Interestingly, Coagulation Factor IX appears to have a median of about 8.8. This trend was not as apparent with applying a transformation. Thus, we have reason to suspect that this protein should be further investigated.

**Temporarily remove the outlier trimming from preprocessing and do some exploratory analysis of the outlying values. Are there specific subjects (not values) that seem to be outliers? If so, are outliers more frequent in one group of the other?**

Through our previous analyses, we see that the distribution of proteins are transformed to easily detect which have a non-normal relationship that can be investigated further. Shifting our focus, we aim to see whether subjects are outliers. That is, we aim to see whether there are subjects, portrayed as rows, that consist of outliers among multiple protein levels.

```{r fig.align='center', warning=FALSE, out.width='80%'}
proteins <- setdiff(names(raw.biomarkers), 
                    c("Group", "Target Full Name"))

z.scores <- raw.biomarkers %>% 
  mutate(across(all_of(proteins), ~scale(as.numeric(.x))))


outlier_summary <- z.scores %>% 
  mutate(across(all_of(proteins), ~abs(.x) > 3)) %>% 
  mutate(n_outliers = rowSums(across(all_of(proteins)), na.rm = TRUE)) %>% 
  filter(!is.na(Group) & Group != '') %>%
  group_by(Group) %>% 
  summarise(
    mean_outliers = mean(n_outliers, na.rm = TRUE),
    median_outliers = median(n_outliers, na.rm = TRUE),
    sd_outliers = sd(n_outliers, na.rm = TRUE),
    max_outliers = max(n_outliers, na.rm = TRUE),
    .groups = "drop"
  )

outlier_summary %>% kable(caption = 'Outlier Distribution Table', digits=2)
```

We created a table to see outlier trends by subject group. To do so, we standardized all 1,317 protein levels and counted values with an absolute value greater than 3 as an outlier. Based on our table, we see that ASD subjects had an average of about 17.04 outliers in protein levels while TD subjects had about 19.65. Additionally, the median outlier counters were nearly identical (9.5 for ASD and 10 for TD), indicating that the two groups have similar overall variation in protein measurements. However, the standard deviations (20.6 for ASD and 30.4 for TD) and maximum outlier counts (119 and 154, respectively) suggest that a few individual subjects in both groups exhibited unusually high numbers of outlying protein values. Overall, this does not suggest there being systematic differences between groups, rather it suggests there are few subjects in the data that have greater individual differences.

### Methodological variations

##### Top 10 Predictive Proteins

To explore the impact of selecting more proteins for the biomarker panel, we expanded the selection criteria to identify the top 20 proteins using three different methods: t-tests for differential expression, Random Forest importance scores, and LASSO regression coefficients. This expansion allows us to evaluate whether including additional proteins could improve classification performance or reveal additional biomarkers of interest.

```{r load-biomarker-data}
load(here("data", "biomarker-clean.RData"))
```

```{r select-expanded-proteins}
top_n <- 20

dat <- biomarker_clean
dat$group <- factor(dat$group)
proteins <- setdiff(names(dat), c('group','ados'))

tt_res <- sapply(proteins, function(p){
  x <- dat[[p]]
  grp <- dat$group
  ok <- !is.na(x) & !is.na(grp)
  if(sum(ok) < 3) return(NA)
  t <- try(t.test(x[ok] ~ grp[ok]), silent=TRUE)
  if(inherits(t,'try-error')) return(NA)
  t$p.value
})
tt_df <- tibble::tibble(protein = proteins,
                       pvalue = as.numeric(tt_res)) %>%
  arrange(pvalue) %>%
  slice_head(n = top_n)

rf_dat <- dat %>% dplyr::select(dplyr::all_of(proteins))
rf_resp <- dat$group
rf_fit <- randomForest::randomForest(x = rf_dat, y = rf_resp, ntree = 1000, importance = TRUE)
rf_imp_mat <- randomForest::importance(rf_fit, type = 2)
rf_imp_val <- if(is.matrix(rf_imp_mat)) rf_imp_mat[,1] else rf_imp_mat
rf_df <- tibble::tibble(protein = names(rf_imp_val), importance = as.numeric(rf_imp_val)) %>%
  arrange(desc(importance)) %>%
  slice_head(n = top_n)

X <- as.matrix(rf_dat)
y <- as.numeric(dat$group) - 1
cv <- glmnet::cv.glmnet(X, y, family = 'binomial', alpha = 1, nfolds = 5)
coef_min <- as.matrix(coef(cv, s = 'lambda.min'))
coefs <- coef_min[-1,1]
lasso_df <- tibble::tibble(protein = proteins, coef = as.numeric(coefs)) %>%
  mutate(abscoef = abs(coef)) %>%
  arrange(desc(abscoef)) %>%
  slice_head(n = top_n)
```

```{r visualize-method-comparison, fig.width=10, fig.height=6}
method_comparison <- bind_rows(
  mutate(tt_df %>% mutate(rank = row_number()), method = "T-test"),
  mutate(rf_df %>% mutate(rank = row_number()), method = "Random Forest"),
  mutate(lasso_df %>% mutate(rank = row_number()), method = "LASSO")
)

ggplot(method_comparison, aes(x = rank, y = protein, color = method)) +
  geom_point(size = 3) +
  theme_minimal() +
  scale_x_continuous(breaks = 1:top_n) +
  labs(title = paste("Top", top_n, "Proteins Selected by Each Method"),
       x = "Rank within Method",
       y = "Protein",
       color = "Selection Method") +
  theme(legend.position = "bottom",
        axis.text.y = element_text(size = 8))
```

```{r protein-overlap}
tt_top <- tt_df$protein
rf_top <- rf_df$protein
lasso_top <- lasso_df$protein

all_methods <- intersect(intersect(tt_top, rf_top), lasso_top)

two_plus_methods <- unique(c(
  intersect(tt_top, rf_top),
  intersect(tt_top, lasso_top),
  intersect(rf_top, lasso_top)
))

overlap_summary <- tibble::tribble(
  ~"Overlap Type", ~"Count", ~"Percentage",
  "Selected by all methods", length(all_methods), length(all_methods)/top_n*100,
  "Selected by ≥2 methods", length(two_plus_methods), length(two_plus_methods)/top_n*100,
  "Unique proteins total", length(unique(c(tt_top, rf_top, lasso_top))), 
  length(unique(c(tt_top, rf_top, lasso_top)))/top_n*100
)

kable(overlap_summary, 
      caption = paste("Overlap Analysis of Top", top_n, "Proteins"),
      digits = 1)
```

Key findings from expanding the protein selection:

1.  **Method Agreement**: Among the top `r top_n` proteins, `r length(all_methods)` were identified by all three methods, suggesting strong consensus on these biomarkers. An additional `r length(two_plus_methods) - length(all_methods)` proteins were selected by at least two methods.

2.  **Unique Contributions**: Each method identified some proteins not found by the others, with a total of `r length(unique(c(tt_top, rf_top, lasso_top)))` unique proteins across all methods. This suggests that different statistical approaches may capture distinct aspects of the protein-ASD relationship.

3.  **Ranking Patterns**: The visualization shows that while some proteins are consistently ranked highly across methods, there is considerable variation in the rankings, particularly for proteins ranked lower in the top `r top_n`.

4.  **Implications**: The expanded panel reveals additional proteins that may be biologically relevant to ASD, though their predictive value would need to be validated in follow-up studies. The overlap analysis suggests that using multiple selection methods provides complementary information about potential biomarkers.

### Simplified classifier

```{r, echo = FALSE, message=FALSE}

kable(bind_rows(metrics_yours, metrics_full), caption = "Training vs Full-Data Performance", digits = 3)
```

To identify a simpler panel while maintaining comparable classification accuracy, we applied a training-only feature selection procedure using the same combination of multiple testing and random forest methods as in the in-class analysis. By restricting selection to the training set, we avoided using any test data during feature selection, ensuring a more methodologically sound and generalizable panel. This yielded a 4-protein panel, compared with the 5-core proteins identified in Hewitson et al.’s original study. The reduction of a single protein is a modest simplification, and despite this, the panel achieved strong predictive performance (sensitivity 0.75, specificity 0.80, accuracy 0.774, AUC 0.871). Benchmarking against the in-class analysis (sensitivity 0.875, specificity 0.867, accuracy 0.871, AUC 0.925) shows that the slight reduction in performance is minor relative to the substantial gain in interpretability and efficiency, supporting the utility of the smaller panel. By performing feature selection exclusively on training data, our panel avoids overfitting to the dataset and is more likely to generalize to new cohorts. Using a smaller panel with fewer proteins has several practical benefits, including cheaper and faster assays in a lab setting, culminating in easier replication.